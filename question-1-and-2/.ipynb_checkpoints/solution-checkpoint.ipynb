{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef7776-0ff7-415c-929a-6fe2438b2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy import sparse\n",
    "from node2vec import Node2Vec\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab63891-98ce-4acf-a1d7-f994a120780c",
   "metadata": {},
   "source": [
    "### 1. Read the ABCD Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e190a0-a241-43a8-9780-0e7c10d5ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_abcd_edges(edgefile):\n",
    "    \"\"\"\n",
    "    Reads 'edge_1000.dat' lines, each 'u v', returning an undirected Graph.\n",
    "    Assumes 0-based node IDs.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with open(edgefile, 'r') as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            u_str, v_str = line.split()\n",
    "            u, v = int(u_str), int(v_str)\n",
    "            G.add_edge(u,v)\n",
    "    return G\n",
    "\n",
    "def read_abcd_communities(comfile):\n",
    "    \"\"\"\n",
    "    Reads 'com_1000.dat': each line 'node community'.\n",
    "    Returns:\n",
    "      - y_true:  array of shape (n,) ground-truth label for each node, ordered by sorted node IDs.\n",
    "      - clusters: a list-of-lists of node IDs that belong to each community.\n",
    "    \"\"\"\n",
    "    node2com = {}\n",
    "    with open(comfile, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            node_id = int(parts[0])\n",
    "            com_id = int(parts[1])\n",
    "            node2com[node_id] = com_id\n",
    "\n",
    "    # build cluster list\n",
    "    com2nodes = defaultdict(list)\n",
    "    for node, com in node2com.items():\n",
    "        com2nodes[com].append(node)\n",
    "    clusters = list(com2nodes.values())\n",
    "\n",
    "    # build y_true using sorted node IDs\n",
    "    sorted_nodes = sorted(node2com.keys())\n",
    "    y_true = np.array([node2com[node] for node in sorted_nodes])\n",
    "\n",
    "    return y_true, clusters, node2com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d657fcb9-92b8-4dfc-8a69-44f77ac7c641",
   "metadata": {},
   "source": [
    "### 2. Global Divergence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b41d6-fdc7-478b-8e44-dc9aa23bf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edge_fractions(G, clusters):\n",
    "    \"\"\"\n",
    "    fraction of edges inside each cluster, and fraction between distinct clusters.\n",
    "    \"\"\"\n",
    "    m = G.number_of_edges()\n",
    "    cluster_of = {}\n",
    "    for i, cset in enumerate(clusters):\n",
    "        for nd in cset:\n",
    "            cluster_of[nd] = i\n",
    "    c_intra = np.zeros(len(clusters))\n",
    "    c_inter_counter = Counter()\n",
    "    \n",
    "    for u,v in G.edges():\n",
    "        cu = cluster_of[u]\n",
    "        cv = cluster_of[v]\n",
    "        if cu==cv:\n",
    "            c_intra[cu]+=1\n",
    "        else:\n",
    "            if cu>cv: cu,cv=cv,cu\n",
    "            c_inter_counter[(cu,cv)]+=1\n",
    "    c_intra /= m\n",
    "    \n",
    "    c_inter_list=[]\n",
    "    for i in range(len(clusters)):\n",
    "        for j in range(i+1,len(clusters)):\n",
    "            c_inter_list.append( c_inter_counter.get((i,j),0)/m )\n",
    "    return c_intra, np.array(c_inter_list)\n",
    "\n",
    "def jensen_shannon_divergence(p, q):\n",
    "    eps=1e-12\n",
    "    p = p+eps\n",
    "    q = q+eps\n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "    m = 0.5*(p+q)\n",
    "    def kl_div(a,b):\n",
    "        return np.sum(a*np.log(a/b))\n",
    "    return 0.5*kl_div(p,m) + 0.5*kl_div(q,m)\n",
    "\n",
    "\n",
    "# approximate geometric chung-lu \n",
    "def fit_geometric_chung_lu(G, clusters, embeddings, epsilon=1.0):\n",
    "    \"\"\"\n",
    "    Probability ~ x[u]*x[v]*g(dist(u,v)), x[u]=sqrt(deg[u]),\n",
    "    g(d)=(1-(d-dmin)/(dmax-dmin))^epsilon\n",
    "    Then sum these probabilities over cluster pairs to get expected fraction.\n",
    "    \"\"\"\n",
    "    nodes = sorted(G.nodes())\n",
    "    deg = np.array([G.degree(nd) for nd in nodes])\n",
    "    x   = np.sqrt(deg)\n",
    "    \n",
    "    coords = np.array([embeddings[nd] for nd in nodes])\n",
    "    # gather all distances\n",
    "    dists=[]\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1,len(nodes)):\n",
    "            dd = np.linalg.norm(coords[i]-coords[j])\n",
    "            dists.append(dd)\n",
    "    dmin, dmax = np.min(dists), np.max(dists)\n",
    "    if abs(dmax - dmin)<1e-12:\n",
    "        dmax=dmin+1e-12\n",
    "    \n",
    "    def gdist(d):\n",
    "        return (1.0 - (d-dmin)/(dmax-dmin))**epsilon\n",
    "    \n",
    "    cluster_of = {}\n",
    "    for i,cset in enumerate(clusters):\n",
    "        for nd in cset:\n",
    "            cluster_of[nd]=i\n",
    "    sum_intra = np.zeros(len(clusters))\n",
    "    sum_inter = Counter()\n",
    "    total_p=0.0\n",
    "    \n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1,len(nodes)):\n",
    "            p_uv = x[i]*x[j]*gdist(np.linalg.norm(coords[i]-coords[j]))\n",
    "            total_p+=p_uv\n",
    "            cu,cv=cluster_of[nodes[i]], cluster_of[nodes[j]]\n",
    "            if cu==cv:\n",
    "                sum_intra[cu]+=p_uv\n",
    "            else:\n",
    "                if cu>cv: cu,cv=cv,cu\n",
    "                sum_inter[(cu,cv)] += p_uv\n",
    "    sum_intra /= total_p\n",
    "    inter_list=[]\n",
    "    for c1 in range(len(clusters)):\n",
    "        for c2 in range(c1+1,len(clusters)):\n",
    "            val=sum_inter.get((c1,c2),0.0)/total_p\n",
    "            inter_list.append(val)\n",
    "    return sum_intra, np.array(inter_list)\n",
    "\n",
    "def global_divergence_score(G, clusters, embeddings):\n",
    "    \"\"\"\n",
    "    Minimizes JSD over a small set of eps in [0.5,1.0,2.0].\n",
    "    \"\"\"\n",
    "    c_intra, c_inter = compute_edge_fractions(G, clusters)\n",
    "    c_vec = np.concatenate([c_intra,c_inter])\n",
    "    best_jsd = float('inf')\n",
    "    \n",
    "    for eps in [0.5,1.0,2.0]:\n",
    "        b_intra, b_inter = fit_geometric_chung_lu(G, clusters, embeddings, eps)\n",
    "        b_vec = np.concatenate([b_intra,b_inter])\n",
    "        jsd_val = jensen_shannon_divergence(c_vec,b_vec)\n",
    "        if jsd_val<best_jsd:\n",
    "            best_jsd=jsd_val\n",
    "    return best_jsd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb76abbf-fee2-4058-95ff-83c39b6e170a",
   "metadata": {},
   "source": [
    "### 3. Two embedding methods: LE & Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ee035-238e-4608-9ad7-4ad219a1172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_eigenmaps_embedding(G, dimension=2):\n",
    "    nodes = sorted(G.nodes())\n",
    "    n = len(nodes)\n",
    "    A = nx.to_scipy_sparse_array(G, nodelist=nodes)  # if new networkx\n",
    "    degs = A.sum(axis=1).flatten()\n",
    "    D_inv_sqrt = sparse.diags(1.0/np.sqrt(degs),0)\n",
    "    I = sparse.eye(n)\n",
    "    L_sym = I - D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    \n",
    "    vals, vecs = eigsh(L_sym, k=dimension+1, which='SM')\n",
    "    order = np.argsort(vals)\n",
    "    vecs = vecs[:,order]\n",
    "    X = vecs[:,1:dimension+1]\n",
    "    emb = {}\n",
    "    for i,node in enumerate(nodes):\n",
    "        emb[node] = X[i].real\n",
    "    return emb\n",
    "\n",
    "def node2vec_embedding(G, dimension=2, walk_length=20, num_walks=10, p=1.0, q=1.0):\n",
    "    n2v = Node2Vec(\n",
    "        G,\n",
    "        dimensions=dimension,\n",
    "        walk_length=walk_length,\n",
    "        num_walks=num_walks,\n",
    "        p=p,\n",
    "        q=q,\n",
    "        seed=random.randint(0,9999999),\n",
    "        workers=1\n",
    "    )\n",
    "    model = n2v.fit(window=5, min_count=1, batch_words=4)\n",
    "    emb={}\n",
    "    for nd in G.nodes():\n",
    "        emb[nd]= model.wv[str(nd)]\n",
    "    return emb\n",
    "\n",
    "def adjusted_graph_rand_index(y_true, y_pred, G):\n",
    "    \"\"\"\n",
    "    Computes Adjusted Graph Rand Index (AGRI) â€” a graph-aware variant of ARI\n",
    "    that only considers pairs of nodes connected by an edge in G.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : list or array of shape (n,)\n",
    "        Ground truth cluster labels (ordered by sorted(G.nodes()))\n",
    "    y_pred : list or array of shape (n,)\n",
    "        Predicted cluster labels (ordered by sorted(G.nodes()))\n",
    "    G : networkx.Graph\n",
    "        The underlying undirected graph (ABCD graph)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    AGRI : float\n",
    "        Adjusted Graph Rand Index (like ARI but on edges)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "    # Get consistent node ordering\n",
    "    nodes = sorted(G.nodes())\n",
    "    node_index = {node: i for i, node in enumerate(nodes)}\n",
    "    \n",
    "    # Gather all edges\n",
    "    pairs = []\n",
    "    for u, v in G.edges():\n",
    "        if u not in node_index or v not in node_index:\n",
    "            continue\n",
    "        i = node_index[u]\n",
    "        j = node_index[v]\n",
    "        pairs.append((i, j))\n",
    "\n",
    "    if len(pairs) == 0:\n",
    "        return 0.0  # No pairs to compare\n",
    "    \n",
    "    # Build binary labels for each pair: same/different in y_true vs y_pred\n",
    "    a = b = c = d = 0\n",
    "    for i, j in pairs:\n",
    "        same_true = y_true[i] == y_true[j]\n",
    "        same_pred = y_pred[i] == y_pred[j]\n",
    "        if same_true and same_pred:\n",
    "            a += 1\n",
    "        elif same_true and not same_pred:\n",
    "            b += 1\n",
    "        elif not same_true and same_pred:\n",
    "            c += 1\n",
    "        else:\n",
    "            d += 1\n",
    "\n",
    "    n = a + b + c + d\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute raw GRI\n",
    "    gri = (a + d) / n\n",
    "\n",
    "    # Estimate expected agreement by chance\n",
    "    p_same_true = (a + b) / n\n",
    "    p_same_pred = (a + c) / n\n",
    "    expected_agreement = p_same_true * p_same_pred + (1 - p_same_true) * (1 - p_same_pred)\n",
    "\n",
    "    if expected_agreement == 1.0:\n",
    "        return 0.0  # Avoid divide by zero\n",
    "\n",
    "    agri = (gri - expected_agreement) / (1 - expected_agreement)\n",
    "    return agri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673c26a0-7dfb-4fe3-b1f2-471adb931d19",
   "metadata": {},
   "source": [
    "### 4. Main experiment loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8f2dff-9dc2-4985-8f81-ed725fd7e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Load ABCD\n",
    "edgefile = 'edge_1000.dat'\n",
    "comfile = 'com_1000.dat'\n",
    "\n",
    "G = read_abcd_edges(edgefile)\n",
    "y_true, clusters, node2com = read_abcd_communities(comfile)\n",
    "print(\"G has\", G.number_of_nodes(), \"nodes and\", G.number_of_edges(), \"edges.\")\n",
    "print(\"Loaded\", len(clusters), \"communities.\")\n",
    "\n",
    "# B) Two embedding methods\n",
    "methods = [\"LE\", \"N2V\"]\n",
    "dims = [2, 4, 8, 16, 32, 64]\n",
    "N_RUNS = 30\n",
    "\n",
    "# We'll store all results in a list of (method,dim,run_id,score)\n",
    "all_runs = []\n",
    "\n",
    "for method in methods:\n",
    "    for d in dims:\n",
    "        for rep in range(N_RUNS):\n",
    "            if method == \"LE\":\n",
    "                emb = laplacian_eigenmaps_embedding(G, d)\n",
    "            else:\n",
    "                # Node2Vec\n",
    "                emb = node2vec_embedding(G, d)\n",
    "\n",
    "            # Evaluate\n",
    "            gscore = global_divergence_score(G, clusters, emb)\n",
    "            all_runs.append((method, d, rep, gscore))\n",
    "\n",
    "# C) Summarize: mean & std for each (method, dim)\n",
    "df = pd.DataFrame(all_runs, columns=[\"method\", \"dim\", \"rep\", \"score\"])\n",
    "agg = df.groupby([\"method\", \"dim\"]).agg(\n",
    "    mean_score=(\"score\", \"mean\"),\n",
    "    std_score=(\"score\", \"std\"),\n",
    "    count=(\"score\", \"count\")\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\n==== Embedding Quality by Global Divergence (lower=better) ====\")\n",
    "print(agg)\n",
    "\n",
    "# Which dimension gave best embeddings? Usually, we look at min mean_score\n",
    "# or stability => min std_score\n",
    "# We'll do that individually:\n",
    "print(\"\\nBest dimension(s) by mean_score, for each method:\")\n",
    "for m in agg[\"method\"].unique():\n",
    "    sub = agg[agg[\"method\"] == m].sort_values(\"mean_score\")\n",
    "    best_row = sub.iloc[0]\n",
    "    print(f\" {m} => dim={best_row['dim']}, mean_score={best_row['mean_score']:.4f} (std={best_row['std_score']:.4f})\")\n",
    "\n",
    "print(\"\\nMost stable dimension (lowest std) for each method:\")\n",
    "for m in agg[\"method\"].unique():\n",
    "    sub = agg[agg[\"method\"] == m].sort_values(\"std_score\")\n",
    "    stbl_row = sub.iloc[0]\n",
    "    print(f\" {m} => dim={stbl_row['dim']}, std_score={stbl_row['std_score']:.4f} (mean={stbl_row['mean_score']:.4f})\")\n",
    "\n",
    "# D) Identify the single best embedding and single worst embedding across all runs\n",
    "best_idx = df[\"score\"].idxmin()\n",
    "worst_idx = df[\"score\"].idxmax()\n",
    "best_row = df.loc[best_idx]\n",
    "worst_row = df.loc[worst_idx]\n",
    "print(\"\\nSingle best embedding:\", best_row)\n",
    "print(\"Single worst embedding:\", worst_row)\n",
    "\n",
    "# E) Re-generate those two embeddings (so we can do KMeans & measure AMI/ARI/AGRI)\n",
    "if best_row[\"method\"] == \"LE\":\n",
    "    best_emb = laplacian_eigenmaps_embedding(G, best_row[\"dim\"])\n",
    "else:\n",
    "    best_emb = node2vec_embedding(G, best_row[\"dim\"])\n",
    "\n",
    "if worst_row[\"method\"] == \"LE\":\n",
    "    worst_emb = laplacian_eigenmaps_embedding(G, worst_row[\"dim\"])\n",
    "else:\n",
    "    worst_emb = node2vec_embedding(G, worst_row[\"dim\"])\n",
    "\n",
    "sorted_nodes = sorted(G.nodes())\n",
    "n = len(sorted_nodes)\n",
    "\n",
    "bestX = np.zeros((n, best_row[\"dim\"]))\n",
    "worstX = np.zeros((n, worst_row[\"dim\"]))\n",
    "\n",
    "# Fill from embedding dicts\n",
    "for i, nd in enumerate(sorted_nodes):\n",
    "    bestX[i] = best_emb[nd]\n",
    "    worstX[i] = worst_emb[nd]\n",
    "\n",
    "y_true = np.array([node2com[node] for node in sorted_nodes])\n",
    "\n",
    "# The known number of communities from ABCD is len(clusters).\n",
    "k_comms = len(clusters)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "# F) K-means on best & worst\n",
    "kmeans_best = KMeans(n_clusters=k_comms, n_init=10, random_state=42).fit(bestX)\n",
    "labs_best = kmeans_best.labels_\n",
    "kmeans_worst = KMeans(n_clusters=k_comms, n_init=10, random_state=42).fit(worstX)\n",
    "labs_worst = kmeans_worst.labels_\n",
    "\n",
    "# G) Compute AMI, ARI, AGRI\n",
    "def all_metrics(y_true, y_pred, G):\n",
    "    ami = adjusted_mutual_info_score(y_true, y_pred)\n",
    "    ari = adjusted_rand_score(y_true, y_pred)\n",
    "    agri = adjusted_graph_rand_index(y_true, y_pred, G)\n",
    "    return ami, ari, agri\n",
    "\n",
    "best_ami, best_ari, best_agri = all_metrics(y_true, labs_best, G)\n",
    "worst_ami, worst_ari, worst_agri = all_metrics(y_true, labs_worst, G)\n",
    "\n",
    "print(\"\\n=== Clustering quality for best embedding ===\")\n",
    "print(f\"AMI={best_ami:.4f}, ARI={best_ari:.4f}, AGRI={best_agri:.4f}\")\n",
    "print(\"=== Clustering quality for worst embedding ===\")\n",
    "print(f\"AMI={worst_ami:.4f}, ARI={worst_ari:.4f}, AGRI={worst_agri:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone Torch",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
